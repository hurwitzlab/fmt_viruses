{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de7bac8",
   "metadata": {},
   "source": [
    "# Quality Control and Trimming\n",
    "\n",
    "This notebook will go through the workflow for read quality control and trimming. We will follow each of the steps below, that will require time on the HPC to run. Be sure to check back after each step to make sure you have the right files, and start the next step.   \n",
    "\n",
    "1. Quality control using fastqc to determine quality thresholds.\n",
    "2. Compressing files before trimming.\n",
    "3. Trimming reads with [Trimmomatic](https://carpentries-lab.github.io/metagenomics-analysis/03-trimming-filtering/index.html).\n",
    "4. Optional: Final QC check after trimming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ebed36",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "You will need to rerun this section each time you come back to this notebook to kick off the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a46521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the variables for your netid and xfile\n",
    "netid = \"kolodisner\"\n",
    "xfile = \"xac\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d388ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go into the working directory\n",
    "work_dir = \"/xdisk/bhurwitz/virus_hunting/\" + netid + \"/02_qc_trimming\"\n",
    "%cd $work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380dffed",
   "metadata": {},
   "source": [
    "## Creating a config file\n",
    "Each of the scripts below executes code that requires certain variables to be set. So we don't need to edit the code in each of the scripts, we are going to use a config file that defines all of these variables. Then when we want to use these variables in the script, we will \"source\" the config file to set the variables. This is generally a good practice in writing scripts on the HPC, that makes it so you only need to modify the config file (rather than each individual script). We are going to create this file using the variables you set above in \"Getting started\". Note that you only need to create this config file once, even if you are returning to complete the next step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "049cf341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a config file with all of the variables you need\n",
    "!echo \"export NETID=$netid\" > config.sh\n",
    "!echo \"export XFILE=$xfile\" >> config.sh\n",
    "!echo \"export XFILE_DIR=/xdisk/bhurwitz/virus_hunting/$netid\" >> config.sh\n",
    "!echo \"export WORK_DIR=/xdisk/bhurwitz/virus_hunting/$netid/02_qc_trimming\" >> config.sh\n",
    "!echo \"export FASTQ_DIR=/xdisk/bhurwitz/virus_hunting/$netid/01_get_fastq\" >> config.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1996dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export NETID=kolodisner\r\n",
      "export XFILE=xac\r\n",
      "export XFILE_DIR=/xdisk/bhurwitz/virus_hunting/kolodisner\r\n",
      "export WORK_DIR=/xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming\r\n",
      "export FASTQ_DIR=/xdisk/bhurwitz/virus_hunting/kolodisner/01_get_fastq\r\n"
     ]
    }
   ],
   "source": [
    "# check the config file to be sure it is correct\n",
    "# Is your netid and xfile correct? Do you have the right directories?\n",
    "!cat config.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeaabfe",
   "metadata": {},
   "source": [
    "## Step 1: Assessing Read Quality\n",
    "\n",
    "Now that we have all of our data downloaded, we are ready to start the quality control process. We will use a tool called fastqc that generates a report about the quality of our sequence data.\n",
    "\n",
    "First, we will create an sbatch script that runs fastqc on each of the sequence files. Note that when you kick off this analysis by running \"sbatch\" below, you will need to wait ~1 hour for the results to come back, depending on the queue wait time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7e3021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script to run fastqc on each of our accessions\n",
    "# A few important points:\n",
    "# 1. We are using the variables from the config file via\n",
    "# the `source ./config.sh` command in the script.\n",
    "# 2. fastqc runs on each of the fastq files in the $FASTQ_DIR\n",
    "# 3. We are creating a directory called check_fastqc in our home directory\n",
    "# this allows us to copy the *html files produced by fastqc and explore\n",
    "# them using Jupyter server on the on demand hpc portal.\n",
    "# 4. The fastqc program runs in the $FASTQ_DIR, but to keep our files\n",
    "# organized, we are going to move the results into our $WORK_DIR.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --array=0-2                         \n",
    "#SBATCH --output=Job-fastqc-%a.out\n",
    "#SBATCH --cpus-per-task=1                  \n",
    "#SBATCH --mem=4G                           \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/fastqc-0.11.9.sif fastqc \\\n",
    "    $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq*\n",
    "\n",
    "mkdir ~/check_fastqc\n",
    "cp $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html ~/check_fastqc \n",
    "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html $WORK_DIR\n",
    "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.zip $WORK_DIR\n",
    " \n",
    "'''\n",
    "\n",
    "with open('run_fastqc.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231b9374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "#SBATCH --ntasks=1\r\n",
      "#SBATCH --nodes=1             \r\n",
      "#SBATCH --time=10:00:00   \r\n",
      "#SBATCH --partition=standard\r\n",
      "#SBATCH --account=bhurwitz\r\n",
      "#SBATCH --array=0-2                         \r\n",
      "#SBATCH --output=Job-fastqc-%a.out\r\n",
      "#SBATCH --cpus-per-task=1                  \r\n",
      "#SBATCH --mem=4G                           \r\n",
      "\r\n",
      "pwd; hostname; date\r\n",
      "\r\n",
      "source ./config.sh\r\n",
      "names=($(cat $XFILE_DIR/$XFILE))\r\n",
      "\r\n",
      "apptainer run /contrib/singularity/shared/bhurwitz/fastqc-0.11.9.sif fastqc     $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq*\r\n",
      "\r\n",
      "mkdir ~/check_fastqc\r\n",
      "cp $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html ~/check_fastqc \r\n",
      "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html $WORK_DIR\r\n",
      "mv $FASTQ_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.zip $WORK_DIR\r\n",
      " \r\n"
     ]
    }
   ],
   "source": [
    "# Check the code and make sure your script above was created.\n",
    "!cat run_fastqc.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9e42306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming\n",
      "ERR2198703_1_fastqc.html  ERR2198705_1_fastqc.html  Job-gzip-1.out\n",
      "ERR2198703_1_fastqc.zip   ERR2198705_1_fastqc.zip   Job-gzip-2.out\n",
      "ERR2198703_2_fastqc.html  ERR2198705_2_fastqc.html  config.sh\n",
      "ERR2198703_2_fastqc.zip   ERR2198705_2_fastqc.zip   run_fastqc.sh\n",
      "ERR2198704_1_fastqc.html  Job-fastqc-0.out\t    run_gzip_untrimmedfiles.sh\n",
      "ERR2198704_1_fastqc.zip   Job-fastqc-1.out\t    trimmed_reads\n",
      "ERR2198704_2_fastqc.html  Job-fastqc-2.out\t    unpaired_reads\n",
      "ERR2198704_2_fastqc.zip   Job-gzip-0.out\n"
     ]
    }
   ],
   "source": [
    "# you should be in your working directory when you run this script\n",
    "# do you see your config.sh file, and the fastqc_parallel.sh script?\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe884278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2973607\r\n"
     ]
    }
   ],
   "source": [
    "# Let's run sbatch to run fastqc on each of the FASTQ files\n",
    "# Remember that this may take 1 hour to run, so take a break, \n",
    "# and get a coffee.\n",
    "!sbatch ./run_fastqc.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3699608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squeue: error: Invalid job id: MY_JOBID\r\n"
     ]
    }
   ],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "# If you get an error \"Invalid job id specified\", then your job already\n",
    "# completed.\n",
    "# You can also check for all jobs under your netid\n",
    "# !squeue --job=$netid\n",
    "!squeue --job=MY_JOBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e70ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR2198703_1_fastqc.html  ERR2198704_2_fastqc.html  Job-fastqc-0.out\n",
      "ERR2198703_1_fastqc.zip   ERR2198704_2_fastqc.zip   Job-fastqc-1.out\n",
      "ERR2198703_2_fastqc.html  ERR2198705_1_fastqc.html  Job-fastqc-2.out\n",
      "ERR2198703_2_fastqc.zip   ERR2198705_1_fastqc.zip   config.sh\n",
      "ERR2198704_1_fastqc.html  ERR2198705_2_fastqc.html  run_fastqc.sh\n",
      "ERR2198704_1_fastqc.zip   ERR2198705_2_fastqc.zip\n",
      "/xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming\n",
      "i16n0.ocelote.hpc.arizona.edu\n",
      "Thu Mar 28 11:15:18 MST 2024\n",
      "Started analysis of ERR2198703_1.fastq\n",
      "Approx 5% complete for ERR2198703_1.fastq\n",
      "Approx 10% complete for ERR2198703_1.fastq\n",
      "Approx 15% complete for ERR2198703_1.fastq\n",
      "Approx 20% complete for ERR2198703_1.fastq\n",
      "Approx 25% complete for ERR2198703_1.fastq\n",
      "Approx 30% complete for ERR2198703_1.fastq\n",
      "Approx 35% complete for ERR2198703_1.fastq\n",
      "Approx 40% complete for ERR2198703_1.fastq\n",
      "Approx 45% complete for ERR2198703_1.fastq\n",
      "Approx 50% complete for ERR2198703_1.fastq\n",
      "Approx 55% complete for ERR2198703_1.fastq\n",
      "Approx 60% complete for ERR2198703_1.fastq\n",
      "Approx 65% complete for ERR2198703_1.fastq\n",
      "Approx 70% complete for ERR2198703_1.fastq\n",
      "Approx 75% complete for ERR2198703_1.fastq\n",
      "Approx 80% complete for ERR2198703_1.fastq\n",
      "Approx 85% complete for ERR2198703_1.fastq\n",
      "Approx 90% complete for ERR2198703_1.fastq\n",
      "Approx 95% complete for ERR2198703_1.fastq\n",
      "Analysis complete for ERR2198703_1.fastq\n",
      "Started analysis of ERR2198703_2.fastq\n",
      "Approx 5% complete for ERR2198703_2.fastq\n",
      "Approx 10% complete for ERR2198703_2.fastq\n",
      "Approx 15% complete for ERR2198703_2.fastq\n",
      "Approx 20% complete for ERR2198703_2.fastq\n",
      "Approx 25% complete for ERR2198703_2.fastq\n",
      "Approx 30% complete for ERR2198703_2.fastq\n",
      "Approx 35% complete for ERR2198703_2.fastq\n",
      "Approx 40% complete for ERR2198703_2.fastq\n",
      "Approx 45% complete for ERR2198703_2.fastq\n",
      "Approx 50% complete for ERR2198703_2.fastq\n",
      "Approx 55% complete for ERR2198703_2.fastq\n",
      "Approx 60% complete for ERR2198703_2.fastq\n",
      "Approx 65% complete for ERR2198703_2.fastq\n",
      "Approx 70% complete for ERR2198703_2.fastq\n",
      "Approx 75% complete for ERR2198703_2.fastq\n",
      "Approx 80% complete for ERR2198703_2.fastq\n",
      "Approx 85% complete for ERR2198703_2.fastq\n",
      "Approx 90% complete for ERR2198703_2.fastq\n",
      "Approx 95% complete for ERR2198703_2.fastq\n",
      "Analysis complete for ERR2198703_2.fastq\n",
      "Detailed performance metrics for this job will be available at https://metrics.hpc.arizona.edu/#job_viewer?action=show&realm=SUPREMM&resource_id=5&local_job_id=2973608 by 8am on 2024/03/29.\n"
     ]
    }
   ],
   "source": [
    "# Once your jobs have run (or are running) you can check the progress\n",
    "# and also look for errors in the *out files\n",
    "# For example, you can look at Job-fastqc-0.out\n",
    "!ls\n",
    "!cat Job-fastqc-0.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89d7e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR2198703_1_fastqc.html  ERR2198704_2_fastqc.html  Job-fastqc-0.out\r\n",
      "ERR2198703_1_fastqc.zip   ERR2198704_2_fastqc.zip   Job-fastqc-1.out\r\n",
      "ERR2198703_2_fastqc.html  ERR2198705_1_fastqc.html  Job-fastqc-2.out\r\n",
      "ERR2198703_2_fastqc.zip   ERR2198705_1_fastqc.zip   config.sh\r\n",
      "ERR2198704_1_fastqc.html  ERR2198705_2_fastqc.html  run_fastqc.sh\r\n",
      "ERR2198704_1_fastqc.zip   ERR2198705_2_fastqc.zip\r\n"
     ]
    }
   ],
   "source": [
    "# Double check that all of your files have run through fastqc.\n",
    "# Do you see a *.html and *.zip file for each one?\n",
    "!ls /xdisk/bhurwitz/virus_hunting/$netid/02_qc_trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b30b51",
   "metadata": {},
   "source": [
    "Great job! It looks like your files have all been checked with fastqc. Before moving on to the next step, start up the Jupyter server on HPC on demand, navigate to the folder called check_fastqc, and double click on each of the html files to check the quality of each of your sequence files. Be sure to refer back to the in-class exercise on quality control to understand what each of the sections means. Because your sequence data come from the SRA, you will likely find that your files are all passing quality control checks already. But, to be certain, we will run a few basic \"screening and cleaning\" steps via trimmomatic in Step 3 to make sure the sequences are up to par. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce6846",
   "metadata": {},
   "source": [
    "## Step 2: Compressing your *fastq files using gzip \n",
    "\n",
    "Trimmomatic works on FASTQ files that are compressed with either gzip or bzip2. So, before we can run trimmomatic, we will need to compress our read files. We'll be using gzip to compress files and get the .gz file extension we need. These FASTQ files are massive, and gzip takes time to run, so lets create a script to sbatch the compression job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bfa8622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a script that gzip's all of the FASTQ files\n",
    "# These are huge files, so it may take 2 hours to run.\n",
    "# This script uses gzip to compress each of the *.fastq files.\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --array=0-2             # the number of accession files\n",
    "#SBATCH --output=Job-gzip-%a.out\n",
    "#SBATCH --cpus-per-task=1        # num CPUs per task\n",
    "#SBATCH --mem=4G                 # total memory per node\n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${XFILE_DIR}/${XFILE}))\n",
    "gzip ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq\n",
    "'''\n",
    "\n",
    "with open('run_gzip_untrimmedfiles.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "069d229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "#SBATCH --ntasks=1\r\n",
      "#SBATCH --nodes=1             \r\n",
      "#SBATCH --time=10:00:00   \r\n",
      "#SBATCH --partition=standard\r\n",
      "#SBATCH --account=bhurwitz\r\n",
      "#SBATCH --array=0-2             # the number of accession files\r\n",
      "#SBATCH --output=Job-gzip-%a.out\r\n",
      "#SBATCH --cpus-per-task=1        # num CPUs per task\r\n",
      "#SBATCH --mem=4G                 # total memory per node\r\n",
      " \r\n",
      "pwd; hostname; date\r\n",
      "source ./config.sh\r\n",
      "names=($(cat ${XFILE_DIR}/${XFILE}))\r\n",
      "gzip ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq\r\n"
     ]
    }
   ],
   "source": [
    "# Let's double check that your script was created.\n",
    "!cat run_gzip_untrimmedfiles.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11d4c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 2982576\r\n"
     ]
    }
   ],
   "source": [
    "# OK, we are ready to kick off the script\n",
    "# Time to go get another coffee...before completing Step 3.\n",
    "!sbatch run_gzip_untrimmedfiles.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f74429b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n",
      "           2982486  standard  jupyter kolodisn  R       5:07      1 i16n0\r\n",
      "           2981033  standard  jupyter kolodisn  R    1:28:15      1 i16n0\r\n"
     ]
    }
   ],
   "source": [
    "# You can check if it is running using the squeue command or looking for\n",
    "# jobs under your netid\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c30a3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR2198703_1_fastqc.html  ERR2198705_1_fastqc.html  Job-gzip-1.out\r\n",
      "ERR2198703_1_fastqc.zip   ERR2198705_1_fastqc.zip   Job-gzip-2.out\r\n",
      "ERR2198703_2_fastqc.html  ERR2198705_2_fastqc.html  config.sh\r\n",
      "ERR2198703_2_fastqc.zip   ERR2198705_2_fastqc.zip   run_fastqc.sh\r\n",
      "ERR2198704_1_fastqc.html  Job-fastqc-0.out\t    run_gzip_untrimmedfiles.sh\r\n",
      "ERR2198704_1_fastqc.zip   Job-fastqc-1.out\t    trimmed_reads\r\n",
      "ERR2198704_2_fastqc.html  Job-fastqc-2.out\t    unpaired_reads\r\n",
      "ERR2198704_2_fastqc.zip   Job-gzip-0.out\r\n"
     ]
    }
   ],
   "source": [
    "# Check to see if all of your *.fastq files are gzip-ed\n",
    "# Note that these files are in the \"02_qc_trimming\" directory\n",
    "# You should just see one gzipped file per fastq\n",
    "# For example, ERR2198631_1.fastq.gz not ERR2198631_1.fastq too...\n",
    "# If you see both .fastq and .fastq.gz the gzip command \n",
    "# is still in progress.\n",
    "!ls /xdisk/bhurwitz/virus_hunting/$netid/02_qc_trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eda481",
   "metadata": {},
   "source": [
    "## Step 3: Trimming .fastq Files\n",
    "\n",
    "In order to run trimmomatic in a PE (paired-end) format we'll need two files. In our case, we have *_1.fastq.gz and *_2.fastq.gz for each accession from the SRA. You should now have those from the steps above.\n",
    "\n",
    "Note that we are following the same trimming protocol from the in-class exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf10aee",
   "metadata": {},
   "source": [
    "### Initial Data Management\n",
    "The output from trimmomatic will give us 4 output files (forward paired, forward unpaired, reverse paired and reverse unpaired. To keep our data organized, let's set up some output directories so the script can organize our data as it runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c27dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trimmed and unpaired directories\n",
    "import os\n",
    "\n",
    "trim_dir = work_dir + \"/trimmed_reads\"\n",
    "unpair_dir = work_dir + \"/unpaired_reads\"\n",
    "\n",
    "if os.path.isdir(trim_dir):\n",
    "    print(\"trim_dir exists\")\n",
    "else:\n",
    "    os.mkdir(trim_dir)\n",
    "\n",
    "if os.path.isdir(unpair_dir):\n",
    "    print(\"unpair_dir exists\")\n",
    "else:\n",
    "    os.mkdir(unpair_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42a6300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to copy the adapter file into your current working directory\n",
    "!cp /xdisk/bhurwitz/virus_hunting/python_notebooks/TruSeq3-PE-2.fa .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e65b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a script that runs trimmomatic on all of our fastq files\n",
    "# you can only run this after the *.fastq files are all gzip-ed (step 2)\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --array=0-2                          # the number of accessions\n",
    "#SBATCH --output=Job-trim-%a.out\n",
    "#SBATCH --cpus-per-task=1                    # num CPUs per task\n",
    "#SBATCH --mem=4G                             # total memory per node\n",
    " \n",
    "pwd; hostname; date\n",
    "source ./config.sh\n",
    "names=($(cat ${XFILE_DIR}/${XFILE}))\n",
    "\n",
    "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\n",
    "UNPAIR_DIR=\"${WORK_DIR}/unpaired_reads\"\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/trimmomatic:0.39--hdfd78af_2.sif trimmomatic PE -phred33 \\\n",
    "    ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz \\\n",
    "    ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz \\\n",
    "    ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz \\\n",
    "    ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10 SLIDINGWINDOW:4:20\n",
    "'''\n",
    "\n",
    "with open('run_trimmomatic_fastq.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8226ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\r\n",
      "#SBATCH --ntasks=1\r\n",
      "#SBATCH --nodes=1             \r\n",
      "#SBATCH --time=10:00:00   \r\n",
      "#SBATCH --partition=standard\r\n",
      "#SBATCH --account=bhurwitz\r\n",
      "#SBATCH --array=0-2                          # the number of accessions\r\n",
      "#SBATCH --output=Job-trim-%a.out\r\n",
      "#SBATCH --cpus-per-task=1                    # num CPUs per task\r\n",
      "#SBATCH --mem=4G                             # total memory per node\r\n",
      " \r\n",
      "pwd; hostname; date\r\n",
      "source ./config.sh\r\n",
      "names=($(cat ${XFILE_DIR}/${XFILE}))\r\n",
      "\r\n",
      "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\r\n",
      "UNPAIR_DIR=\"${WORK_DIR}/unpaired_reads\"\r\n",
      "\r\n",
      "apptainer run /contrib/singularity/shared/bhurwitz/trimmomatic:0.39--hdfd78af_2.sif trimmomatic PE -phred33     ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${FASTQ_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz     ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_1.fastq.gz     ${TRIM_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz ${UNPAIR_DIR}/${names[${SLURM_ARRAY_TASK_ID}]}_2.fastq.gz     ILLUMINACLIP:TruSeq3-PE-2.fa:2:30:10 SLIDINGWINDOW:4:20\r\n"
     ]
    }
   ],
   "source": [
    "# Did you create the script file correctly?\n",
    "!cat run_trimmomatic_fastq.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aa06795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3004157\r\n"
     ]
    }
   ],
   "source": [
    "# Now we can run trimmomatic\n",
    "!sbatch run_trimmomatic_fastq.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69f708e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n",
      "         3004157_0  standard run_trim kolodisn  R       2:59      1 i16n7\r\n",
      "         3004157_1  standard run_trim kolodisn  R       2:59      1 i16n7\r\n",
      "         3004157_2  standard run_trim kolodisn  R       2:59      1 i16n10\r\n",
      "         3003643_0  standard run_trim kolodisn  R      35:34      1 i16n1\r\n",
      "         3003643_1  standard run_trim kolodisn  R      35:34      1 i16n2\r\n",
      "         3003643_2  standard run_trim kolodisn  R      35:34      1 i16n2\r\n",
      "           3003641  standard  jupyter kolodisn  R      40:35      1 i16n2\r\n"
     ]
    }
   ],
   "source": [
    "# You can check if it is running using the squeue command\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb81706",
   "metadata": {},
   "source": [
    "### Checking your output files\n",
    "\n",
    "Once your job has completed, you should see that there are four output files from two input files. The trimmomatic program places all of the \"orphaned\" reads in a separate file from the trimmed reads. Reads can become orphaned when their \"mate pair\" is either too short, or too low quality. For our analyses going forward, we will only use the reads that were trimmed, and have both the forward and reverse read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0ce5573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trimmed:\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 1909334091 Mar 29 12:43 /xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming/trimmed_reads/ERR2198703_1.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 1930343936 Mar 29 12:43 /xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming/trimmed_reads/ERR2198703_2.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 1924374219 Mar 29 12:43 /xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming/trimmed_reads/ERR2198704_1.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 1948849294 Mar 29 12:43 /xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming/trimmed_reads/ERR2198704_2.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 1896715780 Mar 29 12:43 /xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming/trimmed_reads/ERR2198705_1.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 1914595566 Mar 29 12:43 /xdisk/bhurwitz/virus_hunting/kolodisner/02_qc_trimming/trimmed_reads/ERR2198705_2.fastq.gz\n",
      "untrimmed:\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 2394964413 Mar 25 13:40 /xdisk/bhurwitz/virus_hunting/kolodisner/01_get_fastq/ERR2198703_1.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 2447780603 Mar 25 13:40 /xdisk/bhurwitz/virus_hunting/kolodisner/01_get_fastq/ERR2198703_2.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 2801012804 Mar 25 13:42 /xdisk/bhurwitz/virus_hunting/kolodisner/01_get_fastq/ERR2198704_1.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 2875391388 Mar 25 13:42 /xdisk/bhurwitz/virus_hunting/kolodisner/01_get_fastq/ERR2198704_2.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 3144786712 Mar 25 13:43 /xdisk/bhurwitz/virus_hunting/kolodisner/01_get_fastq/ERR2198705_1.fastq.gz\n",
      "-rw-r--r-- 1 kolodisner bhurwitz 3218595647 Mar 25 13:43 /xdisk/bhurwitz/virus_hunting/kolodisner/01_get_fastq/ERR2198705_2.fastq.gz\n"
     ]
    }
   ],
   "source": [
    "# Let's check the file sizes to see that they are smaller for our trimmed\n",
    "# reads\n",
    "!echo \"trimmed:\"\n",
    "!ls -l /xdisk/bhurwitz/virus_hunting/$netid/02_qc_trimming/trimmed_reads/*fastq.gz\n",
    "!echo \"untrimmed:\"\n",
    "!ls -l /xdisk/bhurwitz/virus_hunting/$netid/01_get_fastq/*fastq.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545c522",
   "metadata": {},
   "source": [
    "## Step 4 (optional) QC Final Check\n",
    "\n",
    "If you have any doubts about the trimming process, you can always run fastqc on the trimmed data and double check that you see all \"green\". You can check the fastqc files using Jupyter to check for any failures or other warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a5eb547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are re-running fastqc on the trimmed data\n",
    "my_code = '''#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --nodes=1             \n",
    "#SBATCH --time=10:00:00   \n",
    "#SBATCH --partition=standard\n",
    "#SBATCH --account=bhurwitz\n",
    "#SBATCH --array=0-2                         \n",
    "#SBATCH --output=Job-fastqc-trim-%a.out\n",
    "#SBATCH --cpus-per-task=1                  \n",
    "#SBATCH --mem=4G                           \n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "source ./config.sh\n",
    "names=($(cat $XFILE_DIR/$XFILE))\n",
    "TRIM_DIR=\"${WORK_DIR}/trimmed_reads\"\n",
    "\n",
    "apptainer run /contrib/singularity/shared/bhurwitz/fastqc-0.11.9.sif fastqc \\\n",
    "    $TRIM_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*.fastq*\n",
    "\n",
    "mkdir ~/check_fastqc_trimmed2\n",
    "cp $TRIM_DIR/${names[${SLURM_ARRAY_TASK_ID}]}_*_fastqc.html ~/check_fastqc_trimmed2 \n",
    "\n",
    "'''\n",
    "\n",
    "with open('run_fastqc_trim.sh', mode='w') as file:\n",
    "    file.write(my_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25ca9aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3005522\r\n"
     ]
    }
   ],
   "source": [
    "!sbatch ./run_fastqc_trim.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d039345d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n",
      "           3003641  standard  jupyter kolodisn  R    1:47:53      1 i16n2\r\n"
     ]
    }
   ],
   "source": [
    "# check to see if your job is finished running\n",
    "!squeue --user=$netid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd60c37",
   "metadata": {},
   "source": [
    "Once your job completes, you can look at the *.html files in your home directory in ~/check_fastqc_trimmed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83501ab5",
   "metadata": {},
   "source": [
    "## Final Step\n",
    "Copy your notebook to the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp ~/02_qc_trimming.ipynb $work_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
